{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6391527c-00c4-4eeb-ac83-1c04fdd417cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas.core.algorithms as algos\n",
    "from pandas import Series\n",
    "import scipy.stats.stats as stats\n",
    "import re\n",
    "import traceback\n",
    "import string\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "from itertools import chain\n",
    "\n",
    "class preprocess():\n",
    "    def __init__(self , data , target , weight , labels = None):\n",
    "        self.data = data\n",
    "        self.weight = weight\n",
    "        self.cntlimit = 0.02\n",
    "        self.evntlimit = 0.02\n",
    "        self.target = target\n",
    "        self.X = None\n",
    "        self.Y =  target\n",
    "        self.n = 15\n",
    "        self.df1 = data\n",
    "        self.iv = None\n",
    "        self.iv_df = None\n",
    "        self.new_iv = None\n",
    "        self.fv= None \n",
    "        self.fv1 = None\n",
    "        self.data = data\n",
    "        self.labels = target\n",
    "        self.new_collinear = None        \n",
    "        self.base_features = list(data.columns)\n",
    "        self.one_hot_features = None       \n",
    "        self.record_missing = None\n",
    "        self.record_single_unique = None\n",
    "        self.record_collinear = None\n",
    "        self.record_zero_importance = None\n",
    "        self.record_low_importance = None\n",
    "        self.missing_stats = None\n",
    "        self.unique_stats = None\n",
    "        self.corr_matrix = None\n",
    "        self.feature_importances = None\n",
    "        self.ops = {}\n",
    "        self.one_hot_correlated = False\n",
    "        \n",
    "    def mono_try(self , Y , X , weight ,n , cntlimit , evntlimit): \n",
    "        k = 0\n",
    "        l = 0\n",
    "        r = 0\n",
    "        n1 = 0\n",
    "        flag = 0\n",
    "        self.X = X \n",
    "        self.Y = Y \n",
    "        self.weight = weight \n",
    "        self.cntlimit = cntlimit \n",
    "        self.evntlimit = evntlimit \n",
    "        df1 = pd.DataFrame({\"X\": X, \"Y\": Y , \"weight\":weight})\n",
    "        df1['Y1'] = df1['Y'] * df1['weight']\n",
    "        justmiss = df1[['X','Y','Y1','weight']][df1.X.isnull()]\n",
    "        notmiss = df1[['X','Y','Y1','weight']][df1.X.notnull()]\n",
    "        self.n = n\n",
    "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
    "        if len(justmiss.index) > 0:\n",
    "            d4[\"MAX_VALUE\"] = np.nan\n",
    "            d4[\"COUNT\"] = justmiss.sum().weight\n",
    "            d4[\"EVENT\"] = justmiss.sum().Y1\n",
    "            d4[\"NONEVENT\"] = justmiss.sum().weight - justmiss.sum().Y1\n",
    "        while(np.abs(r) < 1.0 or (np.abs(k) < cntlimit or np.abs(l) < evntlimit)):\n",
    "            if flag > n : \n",
    "                break\n",
    "            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y1,\"weight\":weight, \"Bucket\": pd.qcut(notmiss.X, n , duplicates='drop')})\n",
    "            d2 = d1.groupby('Bucket', as_index=True)\n",
    "            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n",
    "            n = n-1\n",
    "            if len(d2) == 1:\n",
    "                n = 5        \n",
    "                bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n",
    "                if len(np.unique(bins)) == 2:\n",
    "                    bins = np.insert(bins, 0, 1)\n",
    "                    bins[1] = bins[1]-(bins[1]/2)\n",
    "                d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y1,\"weight\":weight, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n",
    "                d2 = d1.groupby('Bucket', as_index=True)\n",
    "                d3 = pd.DataFrame({},index=[])\n",
    "                d3[\"MIN_VALUE\"] = d2.min().X\n",
    "                d3[\"MAX_VALUE\"] = d2.max().X\n",
    "                d3[\"COUNT\"] = d2.sum().weight\n",
    "                d3[\"EVENT\"] = d2.sum().Y\n",
    "                d3[\"NONEVENT\"] = d2.sum().weight - d2.sum().Y\n",
    "                d3=d3.reset_index(drop=True)\n",
    "                if len(d4.index) > 0:\n",
    "                    d3 = d3.append(d4,ignore_index=True)\n",
    "                d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "                d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "                d3[\"DIST_COUNT\"] = d3.COUNT/d3.sum().COUNT\n",
    "                d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "                d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "                d3[\"WOE\"] = np.log(d3.DIST_NON_EVENT/d3.DIST_EVENT)\n",
    "                d3[\"IV_indiv\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*d3['WOE']\n",
    "                d3[\"VAR_NAME\"] = \"VAR\"\n",
    "                d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT','DIST_COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT','NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE','IV_indiv']]                       \n",
    "                d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "                flag +=1 \n",
    "                d3['FORCE'] = 1\n",
    "            else:\n",
    "                d3 = pd.DataFrame({},index=[])\n",
    "                d3[\"MIN_VALUE\"] = d2.min().X\n",
    "                d3[\"MAX_VALUE\"] = d2.max().X\n",
    "                d3[\"COUNT\"] = d2.sum().weight\n",
    "                d3[\"EVENT\"] = d2.sum().Y\n",
    "                d3[\"NONEVENT\"] = d2.sum().weight - d2.sum().Y\n",
    "                d3=d3.reset_index(drop=True)\n",
    "                if(len(d4.index > 0)):\n",
    "                    d3 = d3.append(d4,ignore_index=True)\n",
    "                d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "                d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "                d3[\"DIST_COUNT\"] = d3.COUNT/d3.sum().COUNT\n",
    "                d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "                d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "                d3[\"WOE\"] = np.log(d3.DIST_NON_EVENT/d3.DIST_EVENT)\n",
    "                d3[\"IV_indiv\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*d3['WOE']\n",
    "                d3[\"VAR_NAME\"] = \"VAR\"                     \n",
    "                d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "                l = d3['DIST_EVENT'].min()\n",
    "                k = d3['DIST_COUNT'].min()\n",
    "                d3['FORCE'] = 0\n",
    "        d3['IV'] = (d3['DIST_NON_EVENT'] - d3['DIST_EVENT'])*(np.log(d3.DIST_NON_EVENT/d3.DIST_EVENT))\n",
    "        d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "        d3['IV'] = (d3.IV.sum())\n",
    "        return (d3)\n",
    "    \n",
    "    def char_bin_wt(self , Y, X , weight):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.weight = weight\n",
    "        df1 = pd.DataFrame({\"X\": X, \"Y\": Y , 'weight':weight})\n",
    "        df1['Y1'] = df1['Y'] * df1['weight']\n",
    "        justmiss = df1[['X','Y1','weight']][df1.X.isnull()]\n",
    "        notmiss = df1[['X','Y1','weight']][df1.X.notnull()]    \n",
    "        df2 = notmiss.groupby('X',as_index=True)\n",
    "\n",
    "        d3 = pd.DataFrame({},index=[])\n",
    "        d3[\"COUNT\"] = df2.sum().weight\n",
    "        d3[\"MIN_VALUE\"] = df2.sum().Y1.index\n",
    "        d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n",
    "        d3[\"EVENT\"] = df2.sum().Y1\n",
    "        d3[\"NONEVENT\"] = df2.sum().weight - df2.sum().Y1\n",
    "\n",
    "        if len(justmiss.index) > 0:\n",
    "            d4 = pd.DataFrame({'MIN_VALUE':-np.inf},index=[0])\n",
    "            d4[\"MAX_VALUE\"] = np.nan\n",
    "            d4[\"COUNT\"] = justmiss.sum().weight\n",
    "            d4[\"EVENT\"] = justmiss.sum().Y1\n",
    "            d4[\"NONEVENT\"] = justmiss.sum().weight - justmiss.sum().Y1\n",
    "            d3 = d3.append(d4,ignore_index=True)\n",
    "\n",
    "        d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "        d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "        d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "        d3[\"DIST_COUNT\"] = d3.COUNT/d3.sum().COUNT\n",
    "        d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "        d3[\"WOE\"] = np.log(d3.DIST_NON_EVENT/d3.DIST_EVENT)\n",
    "        d3[\"IV_indiv\"] = (d3.DIST_NON_EVENT-d3.DIST_EVENT)*np.log(d3.DIST_NON_EVENT/d3.DIST_EVENT)\n",
    "        d3[\"IV\"] = (d3.DIST_NON_EVENT-d3.DIST_EVENT)*np.log(d3.DIST_NON_EVENT/d3.DIST_EVENT)\n",
    "        d3[\"VAR_NAME\"] = \"VAR\"\n",
    "        d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "        d3['FORCE'] = 2\n",
    "        d3.IV = d3.IV.sum()\n",
    "        d3 = d3.reset_index(drop=True)\n",
    "        d3 = d3.sort_values(by = 'WOE')\n",
    "        if len(df1['X'].value_counts()) < 3 :\n",
    "            return d3\n",
    "        else :\n",
    "            d3['diff'] = d3['WOE'].sub(d3['WOE'].shift())\n",
    "            d3['diff'] = d3['diff'].fillna(0.0)\n",
    "            d3['diff2'] = d3['diff'].cumsum() * 10\n",
    "            d3['bin'] = (d3['diff2']).astype('int')\n",
    "            d3['MIN_VALUE'] = d3['MIN_VALUE'].astype('str')\n",
    "            grouped_df = d3.groupby(\"bin\")['MIN_VALUE'].apply(lambda x: \"%s\" % ','.join(x))\n",
    "            d_check = d3[['COUNT','EVENT','NONEVENT','bin']].merge(grouped_df.to_frame().reset_index() , on = 'bin')\n",
    "            del d3\n",
    "            d_check = d_check.drop(['bin'],axis = 1)    \n",
    "            df2 = d_check.groupby('MIN_VALUE', as_index = True)\n",
    "            d3 = pd.DataFrame({},index=[])\n",
    "            d3[\"COUNT\"] = df2.sum().COUNT\n",
    "            d3[\"MIN_VALUE\"] = df2.sum().COUNT.index\n",
    "            d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n",
    "            d3[\"EVENT\"] = df2.sum().EVENT\n",
    "            d3[\"NONEVENT\"] = df2.sum().NONEVENT\n",
    "            d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "            d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "            d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "            d3[\"DIST_COUNT\"] = d3.COUNT/d3.sum().COUNT\n",
    "            d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "            d3[\"WOE\"] = np.log(d3.DIST_NON_EVENT/d3.DIST_EVENT)\n",
    "            d3[\"IV_indiv\"] = (d3.DIST_NON_EVENT-d3.DIST_EVENT)*np.log(d3.DIST_NON_EVENT/d3.DIST_EVENT)\n",
    "            d3[\"IV\"] = (d3.DIST_NON_EVENT-d3.DIST_EVENT)*np.log(d3.DIST_NON_EVENT/d3.DIST_EVENT)\n",
    "            d3[\"VAR_NAME\"] = \"VAR\"\n",
    "            d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT','DIST_COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT','NON_EVENT_RATE','DIST_EVENT','DIST_NON_EVENT','WOE', 'IV_indiv' , 'IV']]      \n",
    "            d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "            d3['FORCE'] = 2\n",
    "            d3.IV = d3.IV.sum()\n",
    "            d3 = d3.reset_index(drop=True)\n",
    "            return d3\n",
    "        \n",
    "    def binning(self, evntlimit = 0.03 , cntlimit = 0.03):\n",
    "        self.n = 15\n",
    "        count = 0\n",
    "        iv_df = pd.DataFrame({} , index = [])\n",
    "        for i in list(self.data.columns):\n",
    "            if ((np.issubdtype(self.data[i], np.number)) and (len(Series.unique(self.data[i])) > 2) and (len(self.data[i].value_counts()) > 2)):\n",
    "                conv = self.mono_try(self.target, self.data[i] , self.weight , 15 , cntlimit , evntlimit)\n",
    "                conv[\"VAR_NAME\"] = i\n",
    "                count = count + 1\n",
    "                print(i + ' is completed...')\n",
    "            else:\n",
    "                conv = self.char_bin_wt(self.target, self.data[i] , self.weight)\n",
    "                conv[\"VAR_NAME\"] = i            \n",
    "                count = count + 1\n",
    "                print(i + \" is completed...\")\n",
    "            if count == 0:\n",
    "                iv_df = conv\n",
    "            else:\n",
    "                iv_df = iv_df.append(conv,ignore_index=True)\n",
    "        iv = iv_df[['VAR_NAME','IV']].drop_duplicates().reset_index(drop=True)\n",
    "        iv_df = iv_df[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT','DIST_COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT','NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE','FORCE','IV_indiv','IV']]\n",
    "        self.iv = iv\n",
    "        self.iv_df =  iv_df\n",
    "        return(iv_df  , iv)\n",
    "    \n",
    "    def tidy_split(self , column, sep='|', keep=False):    \n",
    "        df = self.fv1\n",
    "        indexes = list()\n",
    "        new_values = list()\n",
    "        for i, presplit in enumerate(df[column].astype(str)):\n",
    "            values = presplit.split(sep)\n",
    "            if keep and len(values) > 1:\n",
    "                indexes.append(i)\n",
    "                new_values.append(presplit)\n",
    "            for value in values:\n",
    "                indexes.append(i)\n",
    "                new_values.append(value)\n",
    "        new_df = df.iloc[indexes, :].copy()\n",
    "        new_df[column] = new_values\n",
    "        return new_df  \n",
    "    \n",
    "    def preprocess(self):\n",
    "        final_iv = self.iv_df\n",
    "        a = pd.to_numeric(final_iv.MIN_VALUE, errors='coerce')\n",
    "        final_iv  = final_iv[~final_iv['WOE'].isna()]\n",
    "        a = pd.to_numeric(final_iv.MIN_VALUE, errors='coerce')\n",
    "        idx = a.isna()\n",
    "        l1 = list(final_iv[idx]['VAR_NAME'].unique())\n",
    "        fv1 = final_iv[final_iv['VAR_NAME'].isin(l1)]\n",
    "        fv2 = final_iv[~final_iv['VAR_NAME'].isin(l1)]\n",
    "        self.fv1 = fv1 \n",
    "        finaliv2 = self.tidy_split('MIN_VALUE' , ',')\n",
    "        finaliv2['MAX_VALUE']  =  finaliv2['MIN_VALUE']\n",
    "        fv = fv2.append(finaliv2 , ignore_index = True)\n",
    "        self.fv = fv\n",
    "        \n",
    "    def transform(self):\n",
    "        df = self.df1\n",
    "        self.preprocess()\n",
    "        final_iv = self.fv\n",
    "        list_col = list(final_iv.VAR_NAME.unique())\n",
    "        df_t = df[list_col]\n",
    "        for i in list_col:\n",
    "            if df_t[i].dtypes == 'object':\n",
    "                booleanDictionary = {True: 'TRUE', False: 'FALSE'}\n",
    "                df_t[i] = df_t[i].replace(booleanDictionary)\n",
    "                final_i1 = final_iv.loc[final_iv.VAR_NAME == i].reset_index()\n",
    "                final_i1['MAX_VALUE'] = final_i1['MAX_VALUE'].replace(booleanDictionary)\n",
    "                cut1 = np.hstack([final_i1['MAX_VALUE']])\n",
    "                lab = final_i1['WOE'].round(4)\n",
    "                df_t[i] = df_t[i].replace(cut1 , lab)\n",
    "                if(df_t[i].isnull().sum() > 0):\n",
    "                    k = (final_i1[final_i1['MAX_VALUE'].isnull()]['WOE']).values\n",
    "                    df_t[i] = dtst.fillna(k[0])\n",
    "                else:\n",
    "                    df_t[i] = df_t[i]\n",
    "            else :\n",
    "                df_t[i] = df_t[i].astype('float64')\n",
    "                final_i1 = final_iv.loc[final_iv.VAR_NAME == i].reset_index()\n",
    "                final_i1.MAX_VALUE = final_i1.MAX_VALUE.fillna(np.nan).astype(float)\n",
    "                final_i1 = final_i1.sort_values(by = 'MAX_VALUE' , ascending =True).reset_index(drop = True)\n",
    "                if(len(final_iv) == 1) : \n",
    "                    cutoff = np.hstack([np.array(final_i1['MIN_VALUE'][0]), final_i1['MAX_VALUE'].values + 0.0001])\n",
    "                    cutoff = cutoff.astype('float64')\n",
    "                else :     \n",
    "                    if(final_i1['MIN_VALUE'][0] == final_i1['MAX_VALUE'][1]):\n",
    "                        cutoff = np.hstack([final_i1['MAX_VALUE'].values])\n",
    "                        cutoff = cutoff.astype('float64')\n",
    "                    else:\n",
    "                        cutoff = np.hstack([np.array(final_i1['MIN_VALUE'][0]), final_i1['MAX_VALUE'].values + 0.0001])\n",
    "                        cutoff = cutoff.astype('float64')\n",
    "                print(cutoff)\n",
    "                labels = final_i1['WOE']\n",
    "                print(labels)\n",
    "                dtst = pd.cut(df_t[i], bins=cutoff, labels=labels, include_lowest=True , duplicates='drop')\n",
    "                if(df_t[i].isnull().sum()>0):\n",
    "                    k = (final_i1[final_i1['MAX_VALUE'].isnull()]['WOE']).values\n",
    "                    print(k)\n",
    "                    df_t[i] = dtst.fillna(k[0])\n",
    "                else:\n",
    "                    df_t[i] = dtst\n",
    "        return df_t.round(4) \n",
    "\n",
    "    def identify_missing(self, missing_threshold):        \n",
    "        self.missing_threshold = missing_threshold\n",
    "\n",
    "        missing_series = self.data.isnull().sum() / self.data.shape[0]\n",
    "        self.missing_stats = pd.DataFrame(missing_series).rename(columns = {'index': 'feature', 0: 'missing_fraction'})\n",
    "\n",
    "        self.missing_stats = self.missing_stats.sort_values('missing_fraction', ascending = False)\n",
    "\n",
    "        record_missing = pd.DataFrame(missing_series[missing_series > missing_threshold]).reset_index().rename(columns = \n",
    "                                                                                                               {'index': 'feature', \n",
    "                                                                                                                0: 'missing_fraction'})\n",
    "\n",
    "        to_drop = list(record_missing['feature'])\n",
    "\n",
    "        self.record_missing = record_missing\n",
    "        self.ops['missing'] = to_drop\n",
    "        \n",
    "        print('%d features with greater than %0.2f missing values.\\n' % (len(self.ops['missing']), self.missing_threshold))\n",
    "        \n",
    "    def identify_single_unique(self):\n",
    "        unique_counts = self.data.nunique()\n",
    "        self.unique_stats = pd.DataFrame(unique_counts).rename(columns = {'index': 'feature', 0: 'nunique'})\n",
    "        self.unique_stats = self.unique_stats.sort_values('nunique', ascending = True)\n",
    "        \n",
    "        record_single_unique = pd.DataFrame(unique_counts[unique_counts == 1]).reset_index().rename(columns = {'index': 'feature', \n",
    "                                                                                                                0: 'nunique'})\n",
    "\n",
    "        to_drop = list(record_single_unique['feature'])\n",
    "    \n",
    "        self.record_single_unique = record_single_unique\n",
    "        self.ops['single_unique'] = to_drop\n",
    "        \n",
    "        print('%d features with a single unique value.\\n' % len(self.ops['single_unique']))\n",
    "    \n",
    "    def identify_collinear(self, correlation_threshold, one_hot=False):\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "        self.one_hot_correlated = one_hot\n",
    "        \n",
    "        if one_hot:\n",
    "            \n",
    "            features = pd.get_dummies(self.data)\n",
    "            self.one_hot_features = [column for column in features.columns if column not in self.base_features]\n",
    "\n",
    "            self.data_all = pd.concat([features[self.one_hot_features], self.data], axis = 1)\n",
    "            \n",
    "            corr_matrix = pd.get_dummies(features).corr()\n",
    "\n",
    "        else:\n",
    "            corr_matrix = self.data.corr()\n",
    "        \n",
    "        self.corr_matrix = corr_matrix\n",
    "    \n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n",
    "        \n",
    "        to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
    "\n",
    "        record_collinear = pd.DataFrame(columns = ['drop_feature', 'corr_feature', 'corr_value'])\n",
    "\n",
    "        for column in to_drop:\n",
    "\n",
    "            corr_features = list(upper.index[upper[column].abs() > correlation_threshold])\n",
    "\n",
    "            corr_values = list(upper[column][upper[column].abs() > correlation_threshold])\n",
    "            drop_features = [column for _ in range(len(corr_features))]    \n",
    "\n",
    "            temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,\n",
    "                                             'corr_feature': corr_features,\n",
    "                                             'corr_value': corr_values})\n",
    "\n",
    "            record_collinear = record_collinear.append(temp_df, ignore_index = True)\n",
    "\n",
    "        self.record_collinear = record_collinear\n",
    "        self.ops['collinear'] = to_drop\n",
    "        \n",
    "        print('%d features with a correlation magnitude greater than %0.2f.\\n' % (len(self.ops['collinear']), self.correlation_threshold))\n",
    "        \n",
    "    def identify_all(self, selection_params):\n",
    "        self.identify_missing(selection_params['missing_threshold'])\n",
    "        self.identify_single_unique()\n",
    "        self.identify_collinear(selection_params['correlation_threshold'])\n",
    "        self.binning()\n",
    "        self.drop_ivcor()  \n",
    "        self.all_identified = set(list(chain(*list(self.ops.values()))))\n",
    "        self.n_identified = len(self.all_identified)\n",
    "        \n",
    "        \n",
    "    def check_removal(self, keep_one_hot=True):        \n",
    "        self.all_identified = set(list(chain(*list(self.ops.values()))))\n",
    "        print('Total of %d features identified for removal' % len(self.all_identified))\n",
    "        \n",
    "        if not keep_one_hot:\n",
    "            if self.one_hot_features is None:\n",
    "                print('Data has not been one-hot encoded')\n",
    "            else:\n",
    "                one_hot_to_remove = [x for x in self.one_hot_features if x not in self.all_identified]\n",
    "                print('%d additional one-hot features can be removed' % len(one_hot_to_remove))\n",
    "        \n",
    "        return list(self.all_identified)\n",
    "        \n",
    "    \n",
    "    def remove(self, methods, keep_one_hot = True):\n",
    "        features_to_drop = []\n",
    "      \n",
    "        if methods == 'all':\n",
    "            \n",
    "            data = self.data_all\n",
    "                                          \n",
    "            print('{} methods have been run\\n'.format(list(self.ops.keys())))\n",
    "            \n",
    "            features_to_drop = set(list(chain(*list(self.ops.values()))))\n",
    "            \n",
    "        else:\n",
    "            if 'zero_importance' in methods or 'low_importance' in methods or self.one_hot_correlated:\n",
    "                data = self.data_all\n",
    "                \n",
    "            else:\n",
    "                data = self.data\n",
    "                \n",
    "            for method in methods:\n",
    "                \n",
    "                if method not in self.ops.keys():\n",
    "                    raise NotImplementedError('%s method has not been run' % method)\n",
    "                    \n",
    "                else:\n",
    "                    features_to_drop.append(self.ops[method])\n",
    "        \n",
    "            features_to_drop = set(list(chain(*features_to_drop)))\n",
    "            \n",
    "        features_to_drop = list(features_to_drop)\n",
    "            \n",
    "        if not keep_one_hot:\n",
    "            \n",
    "            if self.one_hot_features is None:\n",
    "                print('Data has not been one-hot encoded')\n",
    "            else:\n",
    "                             \n",
    "                features_to_drop = list(set(features_to_drop) | set(self.one_hot_features))\n",
    "       \n",
    "        data = data.drop(columns = features_to_drop)\n",
    "        self.removed_features = features_to_drop\n",
    "        \n",
    "        if not keep_one_hot:\n",
    "            print('Removed %d features including one-hot features.' % len(features_to_drop))\n",
    "        else:\n",
    "            print('Removed %d features.' % len(features_to_drop))\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def plot_missing(self):\n",
    "        \"\"\"Histogram of missing fraction in each feature\"\"\"\n",
    "        if self.record_missing is None:\n",
    "            raise NotImplementedError(\"Missing values have not been calculated. Run `identify_missing`\")\n",
    "        \n",
    "        self.reset_plot()\n",
    "        \n",
    "        plt.style.use('seaborn-white')\n",
    "        plt.figure(figsize = (7, 5))\n",
    "        plt.hist(self.missing_stats['missing_fraction'], bins = np.linspace(0, 1, 11), edgecolor = 'k', color = 'red', linewidth = 1.5)\n",
    "        plt.xticks(np.linspace(0, 1, 11));\n",
    "        plt.xlabel('Missing Fraction', size = 14); plt.ylabel('Count of Features', size = 14); \n",
    "        plt.title(\"Fraction of Missing Values Histogram\", size = 16);\n",
    "        \n",
    "    \n",
    "    def plot_unique(self):\n",
    "        if self.record_single_unique is None:\n",
    "            raise NotImplementedError('Unique values have not been calculated. Run `identify_single_unique`')\n",
    "        \n",
    "        self.reset_plot()\n",
    "\n",
    "        self.unique_stats.plot.hist(edgecolor = 'k', figsize = (7, 5))\n",
    "        plt.ylabel('Frequency', size = 14); plt.xlabel('Unique Values', size = 14); \n",
    "        plt.title('Number of Unique Values Histogram', size = 16);\n",
    "        \n",
    "    \n",
    "    def plot_collinear(self, plot_all = False):\n",
    "        if self.record_collinear is None:\n",
    "            raise NotImplementedError('Collinear features have not been idenfitied. Run `identify_collinear`.')\n",
    "        \n",
    "        if plot_all:\n",
    "            corr_matrix_plot = self.corr_matrix\n",
    "            title = 'All Correlations'\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            corr_matrix_plot = self.corr_matrix.loc[list(set(self.record_collinear['corr_feature'])),                            list(set(self.record_collinear['drop_feature']))]\n",
    "            title = \"Correlations Above Threshold\"\n",
    "\n",
    "       \n",
    "        f, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "        sns.heatmap(corr_matrix_plot, cmap=cmap, center=0,\n",
    "                    linewidths=.25, cbar_kws={\"shrink\": 0.6})\n",
    "\n",
    "        ax.set_yticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[0]))])\n",
    "        ax.set_yticklabels(list(corr_matrix_plot.index), size = int(160 / corr_matrix_plot.shape[0]));\n",
    "\n",
    "        ax.set_xticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[1]))])\n",
    "        ax.set_xticklabels(list(corr_matrix_plot.columns), size = int(160 / corr_matrix_plot.shape[1]));\n",
    "        plt.title(title, size = 14)\n",
    "        \n",
    "    def reset_plot(self):\n",
    "        plt.rcParams = plt.rcParamsDefault\n",
    "        \n",
    "    def drop_ivcor(self):\n",
    "        self.new_collinear = self.record_collinear.merge(self.iv.rename(columns={'VAR_NAME':'drop_feature','IV':'drop_iv'}) , on = 'drop_feature').merge(self.iv.rename(columns={'VAR_NAME':'corr_feature','IV':'corr_iv'}))\n",
    "        self.new_collinear['drop_feature1'] = np.where(self.new_collinear['drop_iv'] < self.new_collinear['corr_iv'] , self.new_collinear['drop_feature'] , self.new_collinear['corr_feature'])\n",
    "        to_drop = list(self.new_collinear['drop_feature1'].unique())\n",
    "        self.ops['IVcollinear'] = to_drop\n",
    "        return list(self.new_collinear['drop_feature1'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "24073af2-62ee-46bc-a468-b283404bd48c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "feature_selection_framework",
   "notebookOrigID": 2551969982924506,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
